<!DOCTYPE html>
<!--
	Interphase by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html lang="en">
	<head>
		<meta charset="UTF-8">
		<title>Espresso</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<script type="text/x-mathjax-config">
  MathJax.Hub.Config({jax: ["output/SVG"], tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
    <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
    <style>
pre .hll { background-color: #ffffcc }
pre  { background: #f0f0f0; }
pre .c { color: #60a0b0; font-style: italic } /* Comment */
pre .err { border: 1px solid #FF0000 } /* Error */
pre .k { color: #007020; font-weight: bold } /* Keyword */
pre .o { color: #666666 } /* Operator */
pre .cm { color: #60a0b0; font-style: italic } /* Comment.Multiline */
pre .cp { color: #007020 } /* Comment.Preproc */
pre .c1 { color: #60a0b0; font-style: italic } /* Comment.Single */
pre .cs { color: #60a0b0; background-color: #fff0f0 } /* Comment.Special */
pre .gd { color: #A00000 } /* Generic.Deleted */
pre .ge { font-style: italic } /* Generic.Emph */
pre .gr { color: #FF0000 } /* Generic.Error */
pre .gh { color: #000080; font-weight: bold } /* Generic.Heading */
pre .gi { color: #00A000 } /* Generic.Inserted */
pre .go { color: #888888 } /* Generic.Output */
pre .gp { color: #c65d09; font-weight: bold } /* Generic.Prompt */
pre .gs { font-weight: bold } /* Generic.Strong */
pre .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
pre .gt { color: #0044DD } /* Generic.Traceback */
pre .kc { color: #007020; font-weight: bold } /* Keyword.Constant */
pre .kd { color: #007020; font-weight: bold } /* Keyword.Declaration */
pre .kn { color: #007020; font-weight: bold } /* Keyword.Namespace */
pre .kp { color: #007020 } /* Keyword.Pseudo */
pre .kr { color: #007020; font-weight: bold } /* Keyword.Reserved */
pre .kt { color: #902000 } /* Keyword.Type */
pre .m { color: #40a070 } /* Literal.Number */
pre .s { color: #4070a0 } /* Literal.String */
pre .na { color: #4070a0 } /* Name.Attribute */
pre .nb { color: #007020 } /* Name.Builtin */
pre .nc { color: #0e84b5; font-weight: bold } /* Name.Class */
pre .no { color: #60add5 } /* Name.Constant */
pre .nd { color: #555555; font-weight: bold } /* Name.Decorator */
pre .ni { color: #d55537; font-weight: bold } /* Name.Entity */
pre .ne { color: #007020 } /* Name.Exception */
pre .nf { color: #06287e } /* Name.Function */
pre .nl { color: #002070; font-weight: bold } /* Name.Label */
pre .nn { color: #0e84b5; font-weight: bold } /* Name.Namespace */
pre .nt { color: #062873; font-weight: bold } /* Name.Tag */
pre .nv { color: #bb60d5 } /* Name.Variable */
pre .ow { color: #007020; font-weight: bold } /* Operator.Word */
pre .w { color: #bbbbbb } /* Text.Whitespace */
pre .mb { color: #40a070 } /* Literal.Number.Bin */
pre .mf { color: #40a070 } /* Literal.Number.Float */
pre .mh { color: #40a070 } /* Literal.Number.Hex */
pre .mi { color: #40a070 } /* Literal.Number.Integer */
pre .mo { color: #40a070 } /* Literal.Number.Oct */
pre .sb { color: #4070a0 } /* Literal.String.Backtick */
pre .sc { color: #4070a0 } /* Literal.String.Char */
pre .sd { color: #4070a0; font-style: italic } /* Literal.String.Doc */
pre .s2 { color: #4070a0 } /* Literal.String.Double */
pre .se { color: #4070a0; font-weight: bold } /* Literal.String.Escape */
pre .sh { color: #4070a0 } /* Literal.String.Heredoc */
pre .si { color: #70a0d0; font-style: italic } /* Literal.String.Interpol */
pre .sx { color: #c65d09 } /* Literal.String.Other */
pre .sr { color: #235388 } /* Literal.String.Regex */
pre .s1 { color: #4070a0 } /* Literal.String.Single */
pre .ss { color: #517918 } /* Literal.String.Symbol */
pre .bp { color: #007020 } /* Name.Builtin.Pseudo */
pre .vc { color: #bb60d5 } /* Name.Variable.Class */
pre .vg { color: #bb60d5 } /* Name.Variable.Global */
pre .vi { color: #bb60d5 } /* Name.Variable.Instance */
pre .il { color: #40a070 } /* Literal.Number.Integer.Long */.syntax pre .hll { background-color: #49483e }
.syntax pre  { background: #272822; color: #f8f8f2 }
.syntax pre .c { color: #75715e } /* Comment */
.syntax pre .err { color: #960050; background-color: #1e0010 } /* Error */
.syntax pre .k { color: #66d9ef } /* Keyword */
.syntax pre .l { color: #ae81ff } /* Literal */
.syntax pre .n { color: #f8f8f2 } /* Name */
.syntax pre .o { color: #f92672 } /* Operator */
.syntax pre .p { color: #f8f8f2 } /* Punctuation */
.syntax pre .cm { color: #75715e } /* Comment.Multiline */
.syntax pre .cp { color: #75715e } /* Comment.Preproc */
.syntax pre .c1 { color: #75715e } /* Comment.Single */
.syntax pre .cs { color: #75715e } /* Comment.Special */
.syntax pre .gd { color: #f92672 } /* Generic.Deleted */
.syntax pre .ge { font-style: italic } /* Generic.Emph */
.syntax pre .gi { color: #a6e22e } /* Generic.Inserted */
.syntax pre .gs { font-weight: bold } /* Generic.Strong */
.syntax pre .gu { color: #75715e } /* Generic.Subheading */
.syntax pre .kc { color: #66d9ef } /* Keyword.Constant */
.syntax pre .kd { color: #66d9ef } /* Keyword.Declaration */
.syntax pre .kn { color: #f92672 } /* Keyword.Namespace */
.syntax pre .kp { color: #66d9ef } /* Keyword.Pseudo */
.syntax pre .kr { color: #66d9ef } /* Keyword.Reserved */
.syntax pre .kt { color: #66d9ef } /* Keyword.Type */
.syntax pre .ld { color: #e6db74 } /* Literal.Date */
.syntax pre .m { color: #ae81ff } /* Literal.Number */
.syntax pre .s { color: #e6db74 } /* Literal.String */
.syntax pre .na { color: #a6e22e } /* Name.Attribute */
.syntax pre .nb { color: #f8f8f2 } /* Name.Builtin */
.syntax pre .nc { color: #a6e22e } /* Name.Class */
.syntax pre .no { color: #66d9ef } /* Name.Constant */
.syntax pre .nd { color: #a6e22e } /* Name.Decorator */
.syntax pre .ni { color: #f8f8f2 } /* Name.Entity */
.syntax pre .ne { color: #a6e22e } /* Name.Exception */
.syntax pre .nf { color: #a6e22e } /* Name.Function */
.syntax pre .nl { color: #f8f8f2 } /* Name.Label */
.syntax pre .nn { color: #f8f8f2 } /* Name.Namespace */
.syntax pre .nx { color: #a6e22e } /* Name.Other */
.syntax pre .py { color: #f8f8f2 } /* Name.Property */
.syntax pre .nt { color: #f92672 } /* Name.Tag */
.syntax pre .nv { color: #f8f8f2 } /* Name.Variable */
.syntax pre .ow { color: #f92672 } /* Operator.Word */
.syntax pre .w { color: #f8f8f2 } /* Text.Whitespace */
.syntax pre .mb { color: #ae81ff } /* Literal.Number.Bin */
.syntax pre .mf { color: #ae81ff } /* Literal.Number.Float */
.syntax pre .mh { color: #ae81ff } /* Literal.Number.Hex */
.syntax pre .mi { color: #ae81ff } /* Literal.Number.Integer */
.syntax pre .mo { color: #ae81ff } /* Literal.Number.Oct */
.syntax pre .sb { color: #e6db74 } /* Literal.String.Backtick */
.syntax pre .sc { color: #e6db74 } /* Literal.String.Char */
.syntax pre .sd { color: #e6db74 } /* Literal.String.Doc */
.syntax pre .s2 { color: #e6db74 } /* Literal.String.Double */
.syntax pre .se { color: #ae81ff } /* Literal.String.Escape */
.syntax pre .sh { color: #e6db74 } /* Literal.String.Heredoc */
.syntax pre .si { color: #e6db74 } /* Literal.String.Interpol */
.syntax pre .sx { color: #e6db74 } /* Literal.String.Other */
.syntax pre .sr { color: #e6db74 } /* Literal.String.Regex */
.syntax pre .s1 { color: #e6db74 } /* Literal.String.Single */
.syntax pre .ss { color: #e6db74 } /* Literal.String.Symbol */
.syntax pre .bp { color: #f8f8f2 } /* Name.Builtin.Pseudo */
.syntax pre .vc { color: #f8f8f2 } /* Name.Variable.Class */
.syntax pre .vg { color: #f8f8f2 } /* Name.Variable.Global */
.syntax pre .vi { color: #f8f8f2 } /* Name.Variable.Instance */
.syntax pre .il { color: #ae81ff } /* Literal.Number.Integer.Long */
    </style>
	</head>
	<body class="landing">

		<!-- Header -->
			<header id="header">
				<h1><a href="index.html">Espresso</a></h1>
				<nav id="nav">
					<ul>
            <li><a href="index.html#writeup">Writeup</a></li>
						<li><a href="proposal.html">Proposal</a></li>
						<li><a href="checkpoint.html">Checkpoint</a></li>
					</ul>
				</nav>
			</header>

		<!-- Banner -->
			<section id="banner">
				<h2>Espresso</h2>
				<p>Speeding up and debittering Caffe by adding Halide</p>
				<ul class="actions">
					<li>
						<a href="https://www.github.com/jczhang/espresso" class="button big">View it on Github</a>
					</li>
				</ul>
			</section>

		<!-- One -->
			<section id="one" class="wrapper style1 align-center">
				<div class="container">
					<header>
						<h2>Summary</h2>
						<p>Espresso is a partial implementation of Caffe in the Halide image processing language. It supports fine-tuning of deep neural networks to achieve high evaluation performance on multiple platforms.</p>
					</header>
					<div class="row 200%">
						<section class="4u 12u$(small)">
							<i class="icon big rounded fa-coffee"></i>
							<p><a href="http://caffe.berkeleyvision.org/">Caffe</a> is a deep learning framework developed by the Berkeley Vision and Learning Center. It features GPU evaluation and training of convolutional neural networks with a clean interface. Models are described by configuration files and trained models can be exported.</p>
						</section>
						<section class="4u 12u$(small)">
							<i class="icon big rounded fa-photo"></i>
							<p><a href="http://halide-lang.org/">Halide</a> is an domain-specific language embedded in C++. It decouples algorithm implementations from scheduling and organization of computation, making it easier to optimize image processing pipelines for performance on multiple platforms.</p>
						</section>
						<section class="4u$ 12u$(small)">
							<i class="icon big rounded fa-angle-double-right"></i>
							<p><a href="https://github.com/jczhang/espresso">Espresso</a> is a deep learning framework implemented with the Halide language and philosophy in mind. We produce highly maintainable yet performant parallel code that is competitive with Caffe and works as a drop-in replacement for Caffe.</p>
						</section>
					</div>
				</div>
			</section>

		<!-- Two -->

			<section id="writeup" class="wrapper">
				<div class="container">

					<header class="major">
						<h2>Espresso</h2>
						<p>Speeding up and debittering Caffe by adding Halide</p>
					</header>

<h2>
<a id="user-content-summary" class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h2>

<p>Espresso is a partial implementation of Caffe in the Halide image processing language. It supports fine-tuning of deep neural networks to achieve high evaluation performance on multiple platforms.</p>

<h2>
<a id="user-content-background" class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h2>

<h3>
<a id="user-content-caffe" class="anchor" href="#caffe" aria-hidden="true"><span class="octicon octicon-link"></span></a>Caffe</h3>

<p>Caffe is a deep learning framework developed by the Berkeley Vision and Learning Center. It features GPU evaluation and training of convolutional neural networks with a clean interface. Models are described by configuration files and trained models can be exported.</p>

<h3>
<a id="user-content-halide" class="anchor" href="#halide" aria-hidden="true"><span class="octicon octicon-link"></span></a>Halide</h3>

<p>Halide is an domain-specific language embedded in C++. It decouples algorithm implementations from scheduling and organization of computation, making it easier to optimize image processing pipelines for performance on multiple platforms.</p>

<h3>
<a id="user-content-caffenet" class="anchor" href="#caffenet" aria-hidden="true"><span class="octicon octicon-link"></span></a>CaffeNet</h3>

              <img class="image fit" src="images/dnn.png" alt="" />


<p>CaffeNet is a 9-layer DNN trained on ILSVRC 2012. It takes <code>256x256x3</code> images as input, and outputs a 1000-wide probability vector, where every item in the vector corresponds with a class. The first five layers are convolutional, and the next three are fully-connected. The last fully-connected layer is connected with a softmax which produces the distribution over the 1000 class labels.</p>

<h2>
<a id="user-content-approach" class="anchor" href="#approach" aria-hidden="true"><span class="octicon octicon-link"></span></a>Approach</h2>

<h3>
<a id="user-content-architecture" class="anchor" href="#architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Architecture</h3>

<p>DNNs in Espresso are expressed as compositions of <a href="https://github.com/jczhang/espresso/include/layer.h"><code>Layer</code></a>s. Every <code>Layer</code> takes a <code>Layer</code> as input, besides the <code>MemoryData Layer</code>, which is used for input. We implemented most of the layers found in Caffe. Input is represented as a four-dimensional matrix <code>(x, y, z, n)</code>, where <code>(x, y)</code> is a single image, <code>z</code> represents the channels in a single image, and <code>n</code> is the image index. For our CaffeNet demo, we take all images in the PPM format from an input directory. By expressing input in this way, we can run multiple images through CaffeNet simultaneously, ensuring that we maximize the amount of work done on the GPU before copying data back out to the CPU. DNNs can be loaded from a <code>.caffemodel</code> file, or built from hand in <code>C++</code>. Each individual layer, as a Halide construct, consists of an algorithm and a schedule. Specifically, every <code>Layer</code> has a <code>Halide::Func forward</code>, which represents the output of the layer. (Currently, Espresso does not support back propogation for training). These functions are defined in the same four dimensions as the input, though they are not constrained in what the dimension sizes actually are.</p>

<h3>
<a id="user-content-scheduling-layers" class="anchor" href="#scheduling-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scheduling Layers</h3>

<p>Scheduling in Halide defines order and locality, and allows us to exploit CPU or GPU parallelism. In Espresso, we aggressively schedule layers to achieve maximum performance. We focused on GPU performance, so our scheduling is done solely through Halide's GPU scheduling primitives. There are a few patterns in scheduling that we found to be very effective. First, tiling pieces of layers and assigning them to thread blocks dramatically sped up computation in most cases, as we were able to exploit both locality and warps not diverging. This pattern works best when the layer requires surrounding pixels to compute, such as in a convolution. Another effective pattern was a call to <code>vectorize()</code>, which is essentially a one-dimensional tile. This pattern works best on layers with only horizontal memory accesses, like our <code>InnerProduct</code> layer. We have provided helper functions for both of these patterns.</p>

<p>Halide aggressively inlines functions in multi-stage pipelines. What this means for Espresso is that if a function is not explicitly scheduled, it will most likely be inlined into the next explictly scheduled step in the Espresso pipeline. This can improve performance and memory usage, since we do not have to store an intermediate image for the inlined function.</p>

<div class="row">
						<section class="feature 6u 12u$(small)">
							<h3 class="title">Halide algorithm</h3>
              <div class="syntax">
<pre style="text-align: left; padding: 1em;"><span class="n">Var</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">l</span><span class="p">;</span>
<span class="n">RDom</span> <span class="nf">r</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">);</span>

<span class="c1">// pad boundaries with 0s</span>
<span class="n">Func</span> <span class="n">clamped</span> <span class="o">=</span>
  <span class="n">BoundaryConditions</span><span class="o">::</span><span class="n">constant_exterior</span><span class="p">(</span>
    <span class="n">input</span><span class="p">,</span> <span class="mf">0.0f</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="p">.</span><span class="n">y</span><span class="p">);</span>
<span class="n">Func</span> <span class="nf">output</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">=</span>
  <span class="n">sum</span><span class="p">(</span><span class="n">clamped</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">r</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="n">r</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">.</span><span class="n">y</span><span class="p">));</span> 
</pre>
</div>
              <p>This snippet of Halide code cleanly describes a simplified convolutional layer with an input image and a kernel. Notice that it follows the mathematical definition of the convolution very closely: \[ (I * K)[i, j] = \sum_m \sum_n I[i-m, j-n]K[m, n] \]</p>
<!--
              <pre style="text-align: left">
Var i, j, k, l;
RDom r(0, kernel_size, 0, kernel_size);

// pad boundaries with 0s
Func clamped =
  BoundaryConditions::constant_exterior(
    input, 0.0f, 0, input.x, 0, input.y);
Func output(i, j) =
  sum(clamped(i - r.x, j - r.y) * kernel(r.x, r.y)); 
</pre>
-->
						</section>

												<section class="feature 6u$ 12u$(small)">

							<h3 class="title">Halide scheduling</h3>
              <div class="syntax">
<pre style="text-align: left; padding: 1em;"><span class="n">Var</span> <span class="n">i_inner</span><span class="p">,</span> <span class="n">i_outer</span><span class="p">,</span> <span class="n">j_inner</span><span class="p">,</span> <span class="n">j_outer</span><span class="p">,</span> <span class="n">tile_index</span><span class="p">;</span>
<span class="n">output</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span>
        <span class="n">i_outer</span><span class="p">,</span> <span class="n">j_outer</span><span class="p">,</span>
        <span class="n">i_inner</span><span class="p">,</span> <span class="n">j_inner</span><span class="p">,</span>
        <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
  <span class="p">.</span><span class="n">unroll</span><span class="p">(</span><span class="n">i_inner</span><span class="p">).</span><span class="n">unroll</span><span class="p">(</span><span class="n">j_inner</span><span class="p">)</span>
  <span class="p">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">i_outer</span><span class="p">,</span> <span class="n">j_outer</span><span class="p">,</span> <span class="n">tile_index</span><span class="p">)</span>
  <span class="p">.</span><span class="n">parallel</span><span class="p">(</span><span class="n">tile_index</span><span class="p">)</span>
  <span class="p">.</span><span class="n">compute_root</span><span class="p">();</span>
</pre>
              <p>This snippet of Halide code describes the allocation of computational resources of the algorithm. Modifying the scheduling to target different architectures will not affect the correctness of the code. Notice that this code splits the output into $4 \times 4$ tiles, unrolls an inner loop computing those tiles, and creates an outer loop indexed by <code>tile_index</code>, which runs in parallel and stores computations to memory.</p>
</div>
<!--
<pre style="text-align: left">
Var i_inner, i_outer, j_inner, j_outer, tile_index;
output.tile(i, j,
        i_outer, j_outer,
        i_inner, j_inner,
        4, 4)
  .unroll(i_inner).unroll(j_inner)
  .fuse(i_outer, j_outer, tile_index)
  .parallel(tile_index)
  .compute_root();
</pre>
-->
						</section>
</div>

<h2>
<a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<h3>
<a id="user-content-performance" class="anchor" href="#performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance</h3>

<p>We tested our implementation of CaffeNet in Espresso using an nVIDIA GTX 770 GPU with 2GB of RAM. Our reference benchmark for both performance and correctness was the same network run in Caffe. We tested on various 256x256x3 images, with a batch size of 30 images on Espresso and 10 images on Caffe. The batch size is larger with Espresso because of Halide's inlining - we manage to save some memory by not storing intermediate layers. We also tested Espresso on a CPU (a Core i7 4770k), but weren't able to test Caffe on a CPU due to compilation issues.</p>


			<span class="image left">
				<img src="images/perf.png">
			</span>
<p>We achieved speeds of about 52ms/image when using a batch size of 30, which remains uncompetitive with Caffe. This is partially due to the fact that Caffe uses cuDNN, a library specifically optimized for neural networks that Halide does not use when compiling (and will not in the forseeable future). We believe that much of the remainder can be made up by further optimizing our layer implementations.</p>

<p>The majority of the computation time was spent in the convolutional layers. Caffe uses a highly tuned matrix multiplication to perform convolution, while our implmentation does the convolution directly. We believe that if we implement convolution through FFTs and matrix multiplication, we could see additional speedup. Unfortunately, we were unable to complete this implementation given the time constraints.</p>

<span class="image left">
<img src="images/batch.png">
</span>

<p>We also tested Espresso on different batch sizes. We found that the more images we ran per batch, the faster the run was per batch. This is because there is overhead in copying the image data from the CPU to the GPU - we noticed little speedup when we increased the batch size when running on the CPU only.</p>

<h3>
<a id="user-content-simplicity" class="anchor" href="#simplicity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simplicity</h3>

<p>We were able to achieve these results with much less time spent optimizing the code than we would have done using Caffe. While not a perfect metric, we can use code size as an estimate for complexity. As shown by the figure, Espressso can define and schedule layers much more concisely than Caffe. Also, we were able to achieve the above performance results with only two contributors over the span of less than a week -- Caffe has been optimized by a hundred contributors over a much longer period.</p>

<span class="image left"><img src="images/codesize.png"></span>

<h3>
<a id="user-content-future-work" class="anchor" href="#future-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Future Work</h3>

<p>If we had more time, we would have attempted to implement training in Halide. This is a more difficult task than testing, because there is backpropagation: the results of every intermediate layer now must be stored and used on the layer above it. This would likely entail splitting the pipeline code into multiple calls to <code>realize()</code>, which would cause some performance loss from the lack of function inlining. However, we would not have to move the buffer back on to the CPU, becuase Halide provides a <code>Buffer</code> type which can exist on the GPU.</p>

<p>We would also focus on bringing performance up to the point where images could be run through the network in less than 10ms. We firmly believe that it is possible with more intelligent scheduling and with more efficient algorithms for some of the layers. 
Because Halide scheduling can be compiled just-in-time, this means that the implementation of each layer has control over when they are inlined into another layer or not. This is a potential area for speedup that cannot be realized in a framework like Caffe, where each layer is a precompiled function. Currently, we take the most composable approach where each layer decides whether to store intermediate results based on its own operations, but more work can be put into determining how to schedule over groups of layers in order to squeeze out more performance.</p>

<h2>
<a id="user-content-references" class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

<p>Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frdo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. In Proceedings of the 34th ACM SIGPLAN conference on Programming language design and implementation (PLDI ’13). ACM, New York, NY, USA, 519-530. DOI=10.1145/2491956.2462176 <a href="http://doi.acm.org/10.1145/2491956.2462176">http://doi.acm.org/10.1145/2491956.2462176</a></p>

<p>Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor. Caffe: Convolutional Architecture for Fast Feature Embedding. 2014. arXiv:1408.5093</p>

<h2>
<a id="user-content-work" class="anchor" href="#work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Work</h2>

<p>Work was divided evenly.</p>
</article>

				</div>


		<!-- Footer -->
			<footer id="footer">
				<div class="container">
					<div class="row">
						<section class="4u 6u(medium) 12u$(small)">
							<h3>Made by</h3>
							<p><a href="https://www.github.com/jczhang">Jeffrey Zhang</a> and <a href="https://www.github.com/iambald">Jeff Chen</a></p>						</section>
						<section class="4u 6u$(medium) 12u$(small)">

							<h3>for</h3>
							<p><a href="http://15418.courses.cs.cmu.edu/spring2015/article/13">the 15-418 Spring 2015 Parallelism Competition</a></p>
						</section>
						<section class="4u$ 12u$(medium) 12u$(small)">
							<h3>with</h3>
              <p><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">&hearts;</a></p>
						</section>
					</div>
					<ul class="copyright">
						<li>Design by <a href="http://templated.co">TEMPLATED</a></li>
						<li>Images by <a href="http://unsplash.com">Unsplash</a></li>
					</ul>
				</div>
			</footer>

	</body>
</html>
